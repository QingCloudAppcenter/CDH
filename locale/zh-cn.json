{
	"Cluster properties" : "集群属性",
	"name" : "名称",
	"The name of the service" : "服务名称",
	"description" : "描述",
	"The description of the service" : "服务描述",
	"VxNet" : "私有网络",
	"Choose a vxnet to join" : "选择应用运行的私有网络",
	"Choose another application service to use, all the nodes in this external service will be added to hosts of current application service" : "选择需要依赖的其他应用服务，该服务里的所有节点将被添加到当前应用服务所有节点的hosts文件里",
	"CPUs of each node" : "每个节点的CPU数量",
	"Memory" : "内存",
	"memory of each node (in MB)" : "每个节点的内存大小（单位MB）",
	"count" : "节点数量",
	"Number of nodes for the cluster to create" : "待创建集群里该类节点数量",
	"instance class" : "节点类型",
	"The instance type for the cluster to run, such as high performance, high performance plus" : "节点分为性能型、超高性能型",
	"volume class" : "数据盘类型",
	"The volume type for each instance, such as high performance, high performance plus" : "数据盘分为性能型、超高性能型",
	"volume size" : "节点容量",
	"The volume size for each instance" : "节点总存储空间大小", 
	"HDFS master" : "HDFS主节点",
        "YARN master" : "YARN主节点",
	"Bigdata client" : "Client节点",
	"HDFS master properties" : "HDFS主节点(Name Node)属性",
	"slave" : "从节点",
	"Slave properties" : "从节点（包含Spark worker, Yarn NodeManager和HDFS Data Node）属性",
	"BigData client" : "BigData client节点",
	"BigData client properties" : "BigData client节点属性",
	"Service properties" : "服务属性",
	"The number of server threads for the data node" : "Data node节点服务线程数",
	"The number of server threads for the name node" : "Name node节点服务线程数",
	"The replication factor in HDFS" : "HDFS副本数",
	"It controls the number of minutes after which a trash checkpoint directory is deleted" : "控制Trash检查点目录过多少分钟后被删除",
	"SparkWorkers" : "Workers(Spark Standalone模式)",
	"SparkApps" : "Applications(Spark Standalone模式)",
	"WorkersTotal" : "Total",
	"WorkersAlive" : "Alive",
	"FilesTotal" : "Total",
	"FilesCreated" : "Created",
	"FilesAppended" : "Appended",
	"FilesRenamed" : "Renamed",
	"FilesDeleted" : "Deleted",
	"RemainingGB" : "Remaining",
	"LiveNodes" : "Live",
	"DeadNodes" : "Dead",
	"DecomLiveNodes" : "DecomLive",
	"DecomDeadNodes" : "DecomDead",
	"DecommissioningNodes" : "Decommissioning",
	"MemUsedMB" : "Used",
	"MemFreeMB" : "Free",
	"BlocksRead" : "Read",
	"BlocksWritten" : "Written",
	"NMMemory" : "NodeManager内存",
	"AllocatedGB" : "Allocated",
	"AvailableGB" : "Available",
	"Compute" : "计算(Spark Standalone)",
	"WorkerMemory" : "内存(Spark Standalone)",
	"Storage" : "存储",
	"Blocks" : "DFS块",
	"Gc" : "垃圾回收",
	"DFS Files" : "DFS文件",
	"DFS Percentage" : "DFS空间占比",
	"DFS Capacity" : "DFS容量",
	"NodeManagers" : "YARN Node Managers",
	"NumActiveNMs" : "Actives",
	"NumDecommissionedNMs" : "Decommissioned",
	"NumLostNMs" : "Lost",
	"NumUnhealthyNMs" : "Unhealthy",
	"NumRebootedNMs" : "Rebooted",
	"Running" : "运行中的YARN应用",
	"running_0" : "小于60分钟",
	"running_60" : "60~300分钟",
	"running_300" : "300~1440分钟",
	"running_1440" : "1440分钟以上",
	"Applications" : "YARN应用",
	"AppsSubmitted" : "Submitted",
	"AppsRunning" : "Running",
	"AppsPending" : "Pending",
	"AppsCompleted" : "Completed",
	"AppsFailed" : "Failed",
	"YarnMemory" : "YARN内存",
	"AllocatedMB" : "Allocated",
	"AvailableMB" : "Available",
	"ReservedMB" : "Reserved",
	"VirtualCores" : "YARN Virtual Cores",
	"AllocatedVCores" : "Allocated",
	"AvailableVCores" : "Available",
	"ReservedVCores" : "Reserved",
	"Containers" : "YARN Containers",
	"AllocatedContainers" : "Allocated",
	"ReservedContainers" : "Reserved",
	"ContainersIniting" : "Initing",
	"ContainersRunning" : "Running",
	"RunningApps" : "Running",
	"WaitingApps" : "Waiting",
	"Hadoop proxy user" : "Hadoop代理用户",
	"Hosts the proxyuser can represent" : "Hadoop代理用户能代理哪些hosts",
	"Groups in hosts the proxyuser can represent" : "Hadoop代理用户能代理指定host中的哪些groups",
	"Enable periodic cleanup of worker/application directories. Only the directories of stopped applications are cleaned up" : "定期清理应用work目录，运行中的application不会被清理。",
	"Controls the interval, in seconds, at which the worker cleans up old application work dirs on the local machine, default to 28800 seconds(8 hours)" : "清理应用work目录的时间间隔，以秒为单位，默认为28800秒（8小时）",
	"The number of seconds to retain application work directories on each worker, default to 86400 seconds(24 hours)" : "保留worker上应用work目录的时间，以秒为单位，默认为86400秒(24 小时)",
	"Memory(in MB) allocated to spark master daemon(standalone mode). The upper limit is total memory - 1024" : "Spark master进程(Standalone模式)占用内存(MB)。该值上限定为总内存-1024",
	"Memory(in MB) allocated to spark worker daemon(standalone mode). The upper limit is total memory - 1024" : "Spark worker进程(Standalone模式)占用内存(MB)。该值上限定为总内存-1024",
	"The maximum amount of heap(in MB) to use by resource manager. It will be reset to current available free memory if 1000 is specified" : "ResourceManager最大可用堆内存大小(MB)，如果指定1000，则ResourceManager将可利用当前所有空闲内存",
	"The maximum amount of heap to use by datanode in MB, Default is 1000. The upper limit is total memory - 1024" : "Datanode daemon进程最大可用堆内存大小(MB)，默认值为1000. 该值上限为总内存-1024",
	"Specify the python version used by a python spark job, current supported python versions are 2.7.13 and 3.6.1. Anaconda distribution data science packages numpy, scikit-learn, scipy, Pandas, NLTK and Matplotlib are also included." : "指定Python Spark程序所用的Python版本，目前支持Anaconda发行版的Python 2.7.13和3.6.1。两个Python版本对应的Anaconda发行版数据科学库numpy, scikit-learn, scipy, Pandas, NLTK和Matplotlib也包含在内",
	"Whether to enable spark standalone mode or not" : "是否开启Spark Standalone模式。开启后将可以以Spark Standalone模式提交Spark应用；关闭后可以以Spark on Yarn模式提交Spark应用；如仅以Spark on YARN模式提交Spark应用或者仅使用hadoop相关功能，则可以选择关闭Spark Standalone模式以释放资源。此选项最好不要和其他配置参数项一起改，单独改动此项然后保存设置是推荐的作法",
	"Whether to use QingStor or not" : "是否将QingStor与Hadoop及Spark集成，如需集成则必须输入相应的access_key及secret_key",
	"Specify QingStor zone, current available zones are pek3a and sh1a" : "指定QingStor的分区，目前开放了pek3a和sh1a, 其他分区何时开放请关注SparkMR用户指南",
	"Scheduler mode within an spark application for parallel jobs that can run simultaneously if they were submitted from separate threads" : "Spark应用内调度模式，针对Spark应用内不同线程提交的可同时运行的任务",
	"The maximum amount of heap(in MB) to use by node manager. The upper limit is half of total memory" : "NodeManager最大可用堆内存大小(MB)，该值上限为总内存的一半",
	"The class to use as the resource scheduler" : "YARN ResourceManager调度器，默认为CapacityScheduler，可选FairScheduler。如果选择FairScheduler，需要上传自定义的fair-scheduler.xml到HDFS的/tmp/hadoop-yarn/目录，然后右键点击集群选择更新调度器。如需对CapacityScheduler的默认行为进行更改，同样需要上传自定义的capacity-scheduler.xml到HDFS的/tmp/hadoop-yarn/目录，然后更新调度器",
	"The number of threads used to handle applications manager requests" : "处理applications manager请求的线程数",
	"Number of threads used to launch/cleanup AM" : "启动/清理ApplicationMaster的线程数",
	"Number of threads to handle scheduler interface" : "处理scheduler接口请求的线程数",
	"Number of threads to handle resource tracker calls" : "处理resource tracker请求的线程数",
	"Number of threads used to handle RM admin interface" : "处理ResourceManager管理接口请求的线程数",
	"Number of threads container manager uses" : "分配给Container Manager用的线程数",
	"Number of threads used in cleanup" : "用于清理工作的线程数",
	"Number of threads to handle localization requests" : "用于处理localization请求的线程数",
	"Number of threads to use for localization fetching" : "用于处理localization fetching请求的线程数",
	"Whether physical memory limits will be enforced for containers." : "是否需要为container检查物理内存限制",
	"Whether virtual memory limits will be enforced for containers." : "是否需要为container检查虚拟内存限制",
	"The ratio of virtual memory to physical memory in node manager" : "NodeManager中虚拟内存与物理内存的比率",
	"The minimum allocation for every container request at the RM, in MBs. Memory requests lower than this will throw a InvalidResourceRequestException" : "ResourceManager中针对每个container请求内存的最小分配值(MB). 低于该值的内存请求将会抛出InvalidResourceRequestException异常",
	"The maximum allocation for every container request at the RM, in MBs. Memory requests higher than this will throw a InvalidResourceRequestException" : "ResourceManager中针对每个container请求内存的最大分配值(MB). 高于该值的内存请求将会抛出InvalidResourceRequestException异常",
	"The minimum allocation for every container request at the RM, in terms of virtual CPU cores. Requests lower than this will throw a InvalidResourceRequestException" : "ResourceManager中针对每个container请求virtual CPU cores的最小分配值。 低于该值的请求将会抛出InvalidResourceRequestException异常",
	"The maximum allocation for every container request at the RM, in terms of virtual CPU cores. Requests higher than this will throw a InvalidResourceRequestException" : "ResourceManager中针对每个container请求virtual CPU cores的最大分配值。 高于该值的请求将会抛出InvalidResourceRequestException异常",
	"Below yarn.scheduler.fair.* options are only valid when FairScheduler is used. Whether to use the username associated with the allocation as the default queue name, in the event that a queue name is not specified. If this is set to false or unset, all jobs have a shared default queue, named default" : "以下yarn.scheduler.fair.*相关选项只有在FairScheduler被使用时才生效。在资源请求中没有指定队列名字的时候，是否使用username作为默认的队列名。如果此选项被设置为false或者未设置，所有job都将共享一个名为default的队列",
	"Whether to use preemption or not" : "是否应用preemption",
	"The utilization threshold after which preemption kicks in. The utilization is computed as the maximum ratio of usage to capacity among all resources" : "超过指定集群资源利用率后将会激活preemption. 资源利用率是已用资源与资源容量的比率",
	"Whether to assign shares to individual apps based on their size, rather than providing an equal share to all apps regardless of size" : "是否根据应用的大小分配资源，而不是对所有应用无视大小分配同样的资源",
	"Whether to allow multiple container assignments in one heartbeat" : "是否允许在一次心跳中指定多个container",
	"If assignmultiple is true, the maximum amount of containers that can be assigned in one heartbeat. Defaults to -1, which sets no limit" : "如果assignmultiple为true，在一次心跳中可指定的最大container数量。设置为-1表示无限制",
	"For applications that request containers on particular nodes, the number of scheduling opportunities since the last container assignment to wait before accepting a placement on another node. Expressed as a float between 0 and 1, which, as a fraction of the cluster size, is the number of scheduling opportunities to pass up. The default value of -1.0 means don’t pass up any scheduling opportunities" : "对于请求某特定节点上container的应用，设定该值指定一个可错失的得到别的节点中container的机会。错失次数超过该值，该请求将得到别的节点的container. 以集群大小百分比的形式指定，-1表示不错失任何调度机会",
	"For applications that request containers on particular racks, the number of scheduling opportunities since the last container assignment to wait before accepting a placement on another rack. Expressed as a float between 0 and 1, which, as a fraction of the cluster size, is the number of scheduling opportunities to pass up. The default value of -1.0 means don’t pass up any scheduling opportunities" : "对于请求某特定rack上container的应用，设定该值指定一个可错失的得到别的rack中container的机会。错失次数超过该值，该请求将得到别的rack的container. 以集群大小百分比的形式指定，-1表示不错失任何调度机会",
	"If this is true, new queues can be created at application submission time, whether because they are specified as the application’s queue by the submitter or because they are placed there by the user-as-default-queue property. If this is false, any time an app would be placed in a queue that is not specified in the allocations file, it is placed in the “default” queue instead. If a queue placement policy is given in the allocations file, this property is ignored" : "如果该值设置为true,每次应用提交后都会创建一个新的队列。如果设置为false，当某应用没有在分配分请求中指定队列的时候，该应用都会被放到default队列中。如果在请求中制定了队列分配策略，则该属性将被忽略",
	"The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption" : "重新锁住调度器重新计算fair shares和请求以及检查是否有资源可以被用于preemption的时间间隔",
	"Whether to enable aggregation log or not" : "是否开启YARN log的集中存储",
	"How long to keep aggregation logs" : "集中存储的log将被保存多久（秒）",
	"How long to wait between aggregated log retention checks. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful set this too small and you will spam the name node." : "多长时间（秒）检查一次集中存储的log是否到期可以清理。如果设置为0或负数，则该值将会被设置为yarn.log-aggregation.retain-seconds的十分之一。如果该值过小可能会导致频繁想name node发送请求",
	"Where to aggregate logs to" : "集中存储的log将被保存在那，默认为HDFS的/tmp/logs目录",
	"The remote log dir will be created at {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}" : "集中存储的log将会被放在{yarn.nodemanager.remote-app-log-dir}/${user}/{本参数}中",
	"update-yarn-scheduler" : "更新调度器"
}
